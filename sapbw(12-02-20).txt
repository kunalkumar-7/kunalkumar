BWHK901031 -tr no.
-------------------------------------------------||from data source to open hub destination||--------------------------------------------
                                           
Data is extracted from the Source Systems.
Data is staged at the Persistent Storage Area(PSA). This holds Source like data.
Data is cleansed, loaded and stored in Data Store Object.
Data is viewed at multiple dimensions in the Infocube.
Data is available by the OLAP processor to the Business Explorer to display data as per Analysis requirements of the Business.
Data can be made available to SAP/Non-SAP, Data Marts by the Open Hub Service.( InfoSpoke).

architecture flow image link : https://www.guru99.com/images/sap/SAP_BI/sap_bi_2_3.jpg


------------------------------------------------------------||question and answer||--------------------
IMPORTANT QUESTIONS &TOPICS:
https://www.erpgreat.com/sap-bw.htm

SAP BW:
https://www.erpgreat.com/index.htm

SAP ARCHITECTURE:
https://www.guru99.com/overview-of-sap-bi-architecture.html

https://vceguide.com/how-are-hierarchies-in-a-bex-query-represented-in-crystal-reports-for-enterprise/

API:
https://help.sap.com/doc/saphelp_nw70/7.0.31/en-US/2e/260a8563b111d4b2ea0050dadfb23f/content.htm?no_cache=true

INDEX/INDICES:
https://blogs.sap.com/2013/08/30/sap-bi-index-usage/

SAP Documentation:
https://help.sap.com/doc/saphelp_nw75/7.5.5/en-US/38/ed95d8780b4d67936b2f25be382a5e/frameset.htm

BW CERTIFICATION:
https://books.google.co.in/books?id=VEGxVL2L0K0C&pg=PA8&lpg=PA8&dq=architecture+of+sap+bw+on+basis+of+extraction,modelling+and+reporting&source=bl&ots=1FAXz4QQTz&sig=ACfU3U2AhcdUKc7DPtEuakwStCHYcO9-Kg&hl=en&sa=X&ved=2ahUKEwjCxMD8i_HnAhWjyDgGHUSkBkcQ6AEwGXoECA0QAQ#v=onepage&q=architecture%20of%20sap%20bw%20on%20basis%20of%20extraction%2Cmodelling%20and%20reporting&f=true


BW CERTIFICATION 2:
https://books.google.co.in/books?id=VEGxVL2L0K0C&lpg=PA8&ots=1FAXz4QQTz&dq=architecture%20of%20sap%20bw%20on%20basis%20of%20extraction%2Cmodelling%20and%20reporting&pg=PA4#v=onepage&q=architecture%20of%20sap%20bw%20on%20basis%20of%20extraction,modelling%20and%20reporting&f=true

SAP HANA:
https://www.youtube.com/watch?v=s-mfDyhmdl8&feature=youtu.be

SAP BW/BI:
https://youtu.be/xSyNOQZrnNI
--------------------------------------------12-02-2020---------------------------------
Standard DSO:
 ZTEST_K7 (info area)
C_ZTESTK71 (data store object)
DESCRIPTION NAME: traning

ztestk71 :Traning_DSO


ZTEST_K7_WO: write optimised


ZTEST_K7_WO :infoarea


ztestk7w:dso


ztestic: infocube





 PROCESS CHAIN

a) RSPC: Process Chain Maintenance

b) RSPC1: Process Chain Display

c) RSPCM: Monitor daily process chains

d) RZ20: To see log for process chains


-BWHCLNT800 :source system
-ZUT_IS_TXT :infosource
-ZPAK_DVELBQ0SYVCIDJZAO7IRWM734 :infopackage
-InfoSource Text(ZUT_IS_TXT) :data source 
-BI Productive client 100(BWHCLNT800) :source system

zpc_18 :process chain tech. name
process_chain_18 :process chain description name
- zpc_81_sp :start process tech. name
-process_chain_start_process: start process desp.name

process chain
-ZINC_LOAD 
-zpc_sam
-z_pc_m


characteristics:z_ch1


key Figures :z_kf9










---------------------------------------------13-02-2020------------------------------
ZCUBE_1 :Training real time (dn)


zut_aggr_text :dtp(data transfer process)


ZMP_14v : multiprovider


zadp_1  : adp(analysis process designer)

---------------------------------------------|| contents of SAP BW ||---------------------------------------
BW system & its version
Transaction Code (T-Code)
OLAP vs OLTP
Modelling:Designing of data base is done by using modelling
PSA
Data Flow
DSO & its types
-standard dso
-write optimised dso
-direct update dso
development of various dso
InfoCubes
- standard cube
-real time cube
- virtual cubes
E-fact,F-fact cube
development of various infocubes
Compression of cubes
Index 
-Primary
-Secondary
    Bit Map
    B-Tree index
partitioning:SAP is using fact table partitioning to improve the performance. you can partition only on 0CALMONTH or 0FISCPER.
-low level
-high level
Data Transfer Process
MultiProvider
-homogenous
-heterogenous
InfoSet
development of infoset
Aggregates
ACR
APD
process chain
types and kinds of data source
Extraction
What are the extractors and mention their types?

To extract data from the system program is used which is known as Extractor. The types of extractors in BW are:

a) Application Specific: BW content FI, HR, CO, SAP CRM, LO cockpit

b) Customer-Generated Extractors: LIS, FI-SL, CO-PA

c) Cross Application (Generic Extractors) : DB View,Infoset, Function Module
Transformation
routine in transformation
logistic data extraction
why lis source is outdated
creation of characteristic and keyfigures
types of characteristics
setup table
attributes
----------------------------------------------------------------||INDEX||---------------------------------------------------------------------

What is meant by Primary Index and Secondary Index

 
Explain the what is primary and secondary index.

When you activate an object say ODS / DSO, the system automatically generate an index based on the key fields and this is primary index.

In addition if you wish to create more indexes , then they are called secondary indexes.

The primary index is distinguished from the secondary indexes of a table. The primary index contains the key fields of the table and a pointer to the non-key fields of the table. The primary index is created automatically when the table is created in the database.

You can also create further indexes on a table. These are called secondary indexes. This is necessary if the table is frequently accessed in a way that does not take advantage of the sorting of the primary index for the access. Different indexes on the same table are distinguished with a three-place index identifier. 

Lets say you have an ODS and the Primary Key is defined as Document Nbr, Cal_day. These two fields insure that the records are unqiue, but lets lay you frequently want to run queries where you selct data based on the Bus Area and Document Type. In this case, we could create a secondary index on Bus Area, Doc Type. Then when the query runs, instead of having to read every record, it can use the index to select records that contain just the Bus Area and Doc type values you are looking for.

Just because you have a secondary index however, does not mean it will be used or should be used. This gets into the cardinality of the fields you are thinking about indexing. For most DBs, an index must be fairly selective to be of any value. That is, given the values you provide in a query for Bus Area and Doc Type, if it will retrieve a very small percentage of the rows form the table, the DB probably should use the index, but if the it would result in retrieving say 40% of the rows, it si almost always better to just read the entire table. 

Having current DB statististics and possibly histograms can be very important as well. The DB statistics hold information on how many distinct values a field has, e.g. how many distinct values of Business Area are there, how many doc types. 

Secondary indexes are usally added to ODS (which you can add using Admin Wkbench) based on your most frequently used queries. Secondary indexes might also be added to selected Dimension and Master data tables as well, but that usually requires a DBA, or someone with similar privileges to create in BW. 


Ahh, I think most of the people thought you were talking about DSO's. A DSO is just a straight table with a primary key. (And you can add secondary indexes by right clicking on "Indexes" at the bottom of the right frame (Below Key Fields and Data Fields and Navigational Attributes). And then dragging the fields you want to make up the secondary index.

For Cubes you have 1 central fact table and many dimension tables that are attached to it. When you create Indexes on a Cube it creates the indexes for all of these tables.

Standard practice in process chains is to have a "Delete Index" job followed by DTP to load the cube followed by "Generate Index" job. (Located under "Data Target Administration drop down in Process Chain maintenance T-Code RSPC)

Doing this should increase performance of queries over this cube.

question:But what is happening if we creat db indexes on InfoCube without delete this indexes before ? The system overwrite infocube indexes ?
answer:Without deleting previous indexes if you create new indexes then old indexes will get overwritten with new index.


-------------------------------------------------------------------------||NEW SAP BW PRACTICAL||----------------------------------------------------------------------
infoarea:

Z_inara : tech. name
infoarea traning: desp.name
BWHK901034 : request ID

dso:
z_dso_t :tech. name
dso traning : desp. name

infocube:
z_incu : tech. name
infocube traning : desp. name





--------------------------------------------------------------------||Diff between DSO vs CUBE||----------------------------------------------------------------

The main difference between the DSO Object and the PSA and InfoCube is that the DSO Object allows existing data to be changed. Whereas an InfoCube principally allows inserts, and only allows deletion on the basis of requests, data in an DSO Object can be changed during the staging process. This enables an DSO Object to be used as a consolidation Object in a Data Warehouse. PSA data change is only supported by manual change or by customer programs and not by the staging mechanism.

Unlike DSO Objects; InfoCubes have a mandatory time-dimension that allows you to look at particular relationships in relation to time-per DSO. For example, you can look at how relationships have changed over a certain time-period.

An DSO Object is principally used for analyzing the status of data at a certain point in time. This allows you to see what relationships are currently like. Exceptionally you can also track history in DSO Objects by adding a date to the key fields of the DSO Object.

It is generally true to say that it is not always necessary to implement DSO Objects in every scenario. Rather, it depends on the requirements of each scenario. You should only use DSO if the requirements of your scenario fit one of the three usage possibilities outlined above (Inbound DSO, Consistent DSO, Application-related DSO). An DSO Object placed in the data flow
to an InfoCube without having a function does nothing except hinder loading performance.

DSO: Data is stored in 2-dimensional format like a regular table.
Cube data is stored as star schema (multi dimensional data)

Cube is used mainly used for analytical reports. DSO is used for operational reports.

DSO has the overwrite capability.
Infocube data will get appended.

DSO gives detailed data, Infocube gives aggregated information

DSO can be used as a datastage and of data consolidation

DSO
1 flat file format structure
2 two dimensional
3 overwrite data functionality
4 performance is less as compared to cube
5 detailed form of data

infocube
1 star schema
2 16 dimentional
3 additive data functionality
4 performance is better as compared to DSO
5 summerised form of data




SAP is using fact table partitioning to improve the performance. you can partition only on 0CALMONTH or 0FISCPER.

infocube uses addition property whereas DSO uses overwrite functionality by default. Generally infocube is used when there are no further loads. Coming to DSO it is used as an intermediatory location to store the data and can collect the data from the difference datasources, used for data cleansing.

cube is additive where as DSO ovewrites by default
cube is for aggregated level of reporting and DSO is for flat data reporting
DSO is nothing but normal two dimensional table where as cube is multidimensional
we need to activate the req to have data in DSO but not in cube
we can maintain aggregates on cube but not on DSO
DSO is mainly used for staging purpose to support deltas where as cube is to do reporting 

Cube,
1. It is Multidimensional
2. It has a Property Additive (This Differs it from DSO)

DSO
1. It is Two Dimensional Table (As in Database Table in Oracle)

Real Time Example

Let consider the following 2 records has come to BW with same comp code and same Item Data but with Different Transaction data to be loaded in Cube and also DSO

Rec CompCode ItemNo NoofItems
1.         1199         677             79
2.         1199         677               0

So Final value should be '0' for Comp Code 1199 and ItemNo 677

Now Lets see what happens in DSO Scenario:

1. When First Record is loaded it will have value like this
Rec CompCode ItemNo NoofItems
1.         1199          677            79
2. When second record come it will be Intenally happining like this
Rec CompCode ItemNo NoofItems
2.         1199        677              0 ( Overriden 79 with 0 )

Now Lets see what happens in CUBE Scenario:

1. When First Record is loaded it will have value like this
Rec CompCode ItemNo NoofItems
1.         1199           677           79
2. When second record come it will be Internally happening like this
Rec CompCode ItemNo NoofItems
1.         1199           677           -79
Since the CUBE will not have property of Override it will pass the value as -79 to make it to zero.

DSO is 2 dimensional table which can store a detailed level of data ie line item lvel.
DSO have overwrite,addition and no update to the fields option.

DSO is mostly used for staging the data from source system and rarely used for report as reports from dso have low perfromance.

Cube is a multidimensional model which have addition and no update option for the key figures.

cubes will have summarized data and is more efficient for reporting.

cube is built in the extended star schema concept which facilitate the sharing of master data.

various performance tuning can be done on cube to improve loading and reporting performance.



--------------------------------------------------------------||DATA SOURCE||----------------------------------------------------------------------
BASIC DEFINATION OF DATA SOURCE:In computer programming, source data or data source is the primary location from where data comes. The data source can be a database, a dataset, a spreadsheet or even hard-coded data. ... A common type of database is an SQL database, but some applications can use other types of databases, like Microsoft Access.

DATA SOURCE/SOURCE SYSTEM:
Any system that is sending data to BW like R/3, flat file, oracle database or external systems.

A DataSource is a set of fields that provide the data for a business unit for data transfer into BI. From a technical viewpoint, the DataSource is a set of logically-related fields that are provided to transfer data into BI in a flat structure (the extraction structure), or in multiple flat structures (for hierarchies).

There are four types of DataSource:

1.DataSource for transaction data
  DataSource for master data
   2.DataSource for attributes
   3.DataSource for texts
   4.DataSource for hierarchies
-----------------------------------------------------------||MULTI PROVIDERS||-------------------------------------------------------------
 Multi-provider is a type of info-provider that contains data from a number of info-providers and makes it available for reporting purposes.

Multi-provider does not contain any data. The data comes entirely from the info providers on which it is based. The info-providers are connected to one another by union operations. Info-providers and Multi-providers are the objects or views relevant for reporting. A multi-provider allows you to run reports using several info-providers that are, it is used for creating reports for one or more than one info-provider at a time.

-------------------------------------------------------||creating multiproviders||---------------------------------------------------------------


Creating MultiProviders 
Prerequisites
There is an active version of each InfoObject that you want to transfer to the MultiProvider. Create any InfoObjects that you require that do not already exist and activate them.

Instead of creating a new MultiProvider, you can install a MultiProvider from SAP Business Content.

Procedure
       1.      Create an InfoArea to which you want to assign the new MultiProvider.

                 Choose Modeling → InfoProvider.

       2.      In the context menu of the InfoArea, choose Create MultiProvider.

       3.      Enter a technical name and a description.

       4.      Choose This graphic is explained in the accompanying text Create.

       5.      Select the InfoProvider that you want to form the MultiProvider. Choose This graphic is explained in the accompanying text Continue. The MultiProvider                screen appears.

       6.      Use drag and drop to transfer the required InfoObjects into your MultiProvider. You can also transfer entire dimensions.

       7.      Use Identify Characteristics and Select Key Figures to make InfoObject assignments between MultiProviders and InfoProviders.

Note

In a MultiProvider, each InfoObject in the MultiProvider must correspond to exactly one InfoObject in each of the InfoProviders involved (as long as it is available in the MultiProvider). If this mapping is not clear, you have to specify the InfoObject to which you want to assign the InfoObject in the MultiProvider.

See also, Consistency Check for Compounding.

       8.      Save or Activate the MultiProvider. Only active MultiProviders are available for analysis and reporting.

See also:

The additional functions in DataStore object maintenance are also available as additional functions in MultiProvider maintenance. The only exception is the last function listed for performance settings.

 

End of Content Area







------------------------------------------------------------------||                 ||------------------------------------------------------
BEx Query Designer
Integrated Planning
Source Systems
Info Provider
Data Warehousing
Data Modeling
BEx Broadcaster
Scheduling & Monitoring
Performance Tuning
Development Interfaces
Documents
Meta Data
Analysis Process Designer
Info Objects
Data Flow
BEx Analyzer
BEx Report Designer

-------------------------------------------------------------------||logistic vs finance data source||---------------------------------------------
In LO extraction delta flow as follows

SM13---- LBWQ ---- RSA7

In FI

SM13 ---- RSA7

so in LO extraction there is one additional data queue as shown above..

In FI Extraction............data directly comes from the Application tables..........But in case of LO Extraction...........Application tables cannot be access directly............first we need to fill the Set up table...........and the full update will extract data from this set up table...........due to this ur full load is not picking any records...............after filling the set up table............check in RSA3.........whether it contains records or not.........

For filling set up table...........Tcode : SBIW...........Settings for Application-Specific DataSources (PI)>>Logistics >>Managing Extract Structures >>Initialization >>Filling in the Setup Table >>Application-Specific Setup of Statistical Data >> in that Youcan perform the Setup (Example : SD-Sales Orders - Perform Setup) and execute ...........T Code : OLI*BW * equals to application area number like 02 for purchasing....................


Lo extraction for the full load data is taken from SETUP tables to BW when we run Full load.And the deltas are from DeltaQueue.
FI full load is updated from R/3 system & FI deltas are updated from RSA7 to the BW system.


REP_20200226053930: new query on 26feb2020






---------------------------------------------------------------------------------------------------------------------------------------------
Processing Type for BEx Report:
1)Manual entry or default value: The variables that will take the default values of characteristics or the value sholud be entered manually by the user
2)Replacement path:The variable value get will replaced by someother characteristic or by some other variable or by some other query
3)Authorization:The variable value can be given by authorised person
4)Customer exit:The variable value by the user 
5)SAP exit:The variable is defined by the SAP